{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCRAM Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:37:51.973404Z",
     "start_time": "2023-04-05T16:37:51.483749Z"
    }
   },
   "outputs": [],
   "source": [
    "import pycram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bullet World\n",
    "\n",
    "The BulletWorld is the internal simulation of PyCRAM. You can simulate different actions and reason about the outcome of different actions. \n",
    "\n",
    "It is possible to spawn objects and robots into the BulletWorld, these objects can come from URDF, OBJ or STL files. \n",
    "\n",
    "A BulletWorld can be created by simply creating an object of the BulletWorld class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:37:59.788099Z",
     "start_time": "2023-04-05T16:37:57.586879Z"
    }
   },
   "outputs": [],
   "source": [
    "from pycram.bullet_world import BulletWorld, Object\n",
    "\n",
    "world = BulletWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The world can also be closed with the 'exit' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:38:04.800655Z",
     "start_time": "2023-04-05T16:38:03.943911Z"
    }
   },
   "outputs": [],
   "source": [
    "world.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BulletWorld allows to render images from arbitrary positoins. In the following example we render images with the camera at the position [0.3, 0, 1] and pointing towards [1, 0, 1], so we are looking upwards along the x-axis. \n",
    "\n",
    "The renderer returns 3 different kinds of images which are also shown at the left side of the BulletWorld window. These images are:\n",
    "* A RGB image which shows everything like it is rendered in the BulletWorld window, just from another perspective. \n",
    "* A depth image which consists of distance values from the camera towards the objects in the field of view. \n",
    "* A segmentation mask image which segments the image into the different objects displayed. The segmentation is done by assigning every pixel the unique id of the object that is displayed there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.bullet_world_reasoning import _get_images_for_target\n",
    "\n",
    "_get_images_for_target([[1, 0, 1], [0, 0, 0, 1]], [[0.3, 0, 1], [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objects\n",
    "Everything that is located inside the BulletWorld is an Object. \n",
    "Objects can be created from URDF, OBJ or STL files. Since everything is of type Object a robot might share the same methods as a milk (with some limitations).\n",
    "\n",
    "Signature:\n",
    "Object:\n",
    "* Name \n",
    "* Type\n",
    "* Filename or Filepath\n",
    "\n",
    " Optional:\n",
    " * Position\n",
    " * Orientation\n",
    " * World \n",
    " * Color \n",
    " * Ignore Cached Files\n",
    "\n",
    "If there is only a filename and no path PyCRAM will check in the resource directory if there is a matching file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk = Object(\"Milk\", \"milk\", \"milk.stl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects provide methods to change the position and rotation, change the color, attach other objects, set the state of joints if the objects has any or get the position and orientation of a link. \n",
    "\n",
    "These methods are the same for every Object, however since some Objects may not have joints or more than one link methods related to these will not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk.set_position([1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove an Object from the BulletWorld just call the 'remove' method on the Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since everything inside the BulletWorld is an Object, even a complex environment Object like the kitchen can be spawned in the same way as the milk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitchen = Object(\"kitchen\", \"environment\", \"kitchen.urdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costmaps\n",
    "\n",
    "Costmaps are a way to get positions with respect to certain criterias. \n",
    "The currently available costmaps are:\n",
    "* Occupancy Costmap\n",
    "* Visibility Costmap\n",
    "* Semantic Costmap \n",
    "* Gaussian Costmap\n",
    "\n",
    "It is also possible to merge multiple costmaps to combine different criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visibility Costmaps\n",
    "Visibility costmaps determine every position, around a target position, from which the target is visible. Visibility Costmaps are able to work with cameras that are moveable in height, for example if the robot has a movable torso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycram.costmaps as cm\n",
    "v = cm.VisibilityCostmap(1.27, 1.60, size=300, resolution=0.02, origin=[[0, 0, 0.1], [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.close_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupancy Costmap\n",
    "Is valid for every position where the robot can be placed without colliding with an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = cm.OccupancyCostmap(0.2, from_ros=False, size=300, resolution=0.02, origin=[[0, 0, 0.1], [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = cm.SemanticCostmap(kitchen, \"kitchen_island_surface\", size=100, resolution=0.02)\n",
    "\n",
    "g = cm.GaussianCostmap(200, 15, resolution=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the costmap in the BulletWorld to get an impression what information is actually contained in the costmap. With this you could also check if the costmap was created correctly. \n",
    "Visualization can be done via the 'visualize' method of each costmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.close_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to combine two costmap, this will result in a new costmap with the same size which contains the information of both previous costmaps. Combination is done by checking for each position in the two costmaps if they are zero, in this case to same position in the new costmap will also be zero in any other case the new position will be the normalized product of the two combined costmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov = o + v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.close_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bullet World Reasoning \n",
    "Allows for geometric reasoning in the BulletWorld. At the moment the following types of reasoning are supported:\n",
    "* Stable\n",
    "* Contact\n",
    "* Visible \n",
    "* Occluding \n",
    "* Reachable \n",
    "* Blocking\n",
    "* Supporting\n",
    "\n",
    "To show the geometric reasoning we first a robot as well as the milk Object again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycram.bullet_world_reasoning as btr\n",
    "milk = Object(\"Milk\", \"milk\", \"milk.stl\", position=[1, 0, 1])\n",
    "pr2 = Object(\"pr2\", \"robot\", \"pr2.urdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with testing for visibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "milk.set_position([1,0,1])\n",
    "visible = btr.visible(milk, pr2.get_link_position_and_orientation(\"wide_stereo_optical_frame\"))\n",
    "print(f\"Milk visible: {visible}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk.set_position([1, 0, 0.05])\n",
    "\n",
    "plane = BulletWorld.current_bullet_world.objects[0]\n",
    "contact = btr.contact(milk, plane)\n",
    "print(f\"Milk is in contact with the floor: {contact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk.set_position([0.6, -0.5, 0.7])\n",
    "\n",
    "reachable = btr.reachable(milk, pr2, \"r_gripper_tool_frame\")\n",
    "print(f\"Milk is reachable for the PR2: {reachable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designator\n",
    "Designator are symbolic descriptions of Actions, Motions, Objects or Locations. In PyCRAM the different types of designators are representet by a class which takes a description, the description then tells the designator what to do. \n",
    "\n",
    "For example, let's look at a Motion Designator to move the robot to a specific location. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Designator\n",
    "\n",
    "When using a Motion Designator you need to specify which Process Module needs to be used, either the Process Module for the real or the simulated robot. This can be done either with a decorator which can be added to a function and then executes every designator in this function on the specified robot. The other possibility is a \"with\" scope which wraps a code piece. \n",
    "\n",
    "These two ways can also be combined, you could write a function which should be executed on the real robot and in the function is a \"with\" scope which executes something on the simulated robot for reasoning purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.motion_designator import *\n",
    "from pycram.process_module import simulated_robot, with_simulated_robot\n",
    "\n",
    "description = MoveMotion(target=[[1, 0, 0], [0, 0, 0, 1]])\n",
    "\n",
    "with simulated_robot:\n",
    "    description.resolve().perform()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pycram.process_module import with_simulated_robot\n",
    "\n",
    "@with_simulated_robot\n",
    "def move():\n",
    "    MoveMotion(target=[[0, 0, 0], [0, 0, 0, 1]]).resolve().perform()\n",
    "\n",
    "move()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other implemented Motion Designator descriptions are:\n",
    "* Pick up\n",
    "* Place\n",
    "* Accessing\n",
    "* Move TCP\n",
    "* Looking\n",
    "* Move Gripper\n",
    "* Detecting\n",
    "* Move Arm Joint \n",
    "* World State Detecting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Designator\n",
    "\n",
    "Object Designator represent objects, these objects could either be from the BulletWorld or the real world. Object Designator are used, for example, by the PickUpAction to know which object should be picked up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.object_designator import *\n",
    "\n",
    "milk_desig = BelieveObject(names=[\"Milk\"])\n",
    "milk_desig.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Designator\n",
    "Location Designator can create a position in cartisian space from a symbolic desctiption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.location_designator import *\n",
    "from pycram.designators.object_designator import *\n",
    "\n",
    "robot_desig = BelieveObject(types=[\"robot\"]).resolve()\n",
    "milk_desig = BelieveObject(names=[\"Milk\"]).resolve()\n",
    "location_desig = CostmapLocation(target=milk_desig, visible_for=robot_desig)\n",
    "\n",
    "print(f\"Resolved: {location_desig.resolve()}\")\n",
    "print()\n",
    "\n",
    "for pose in location_desig:\n",
    "    print(pose)\n",
    "                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Designator\n",
    "Action Designator are used to describe high-level actions. Action Designator are usually composed of other Designators to describe the high-level action in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.action_designator import *\n",
    "from pycram.enums import Arms\n",
    "\n",
    "with simulated_robot:\n",
    "    ParkArmsAction([Arms.BOTH]).resolve().perform()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a simple plan\n",
    "To get familiar with the PyCRAM Framework we will write a simple pick and place plan. This plan will let the robot grab a cereal box from the kitchen counter and place it on the kitchen island. This is a simple pick and place plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.object_designator import *\n",
    "cereal = Object(\"cereal\", \"cereal\", \"breakfast_cereal.stl\", position=[1.4, 1, 0.95])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cereal_desig = ObjectDesignatorDescription(names=[\"cereal\"])\n",
    "kitchen_desig = ObjectDesignatorDescription(names=[\"kitchen\"])\n",
    "robot_desig = ObjectDesignatorDescription(names=[\"pr2\"]).resolve()\n",
    "with simulated_robot:\n",
    "    ParkArmsAction([Arms.BOTH]).resolve().perform()\n",
    "\n",
    "    MoveTorsoAction([0.3]).resolve().perform()\n",
    "\n",
    "    pickup_pose = CostmapLocation(target=cereal_desig.resolve(), reachable_for=robot_desig).resolve()\n",
    "    pickup_arm = pickup_pose.reachable_arms[0]\n",
    "\n",
    "    NavigateAction(target_locations=[pickup_pose.pose]).resolve().perform()\n",
    "\n",
    "    PickUpAction(object_designator_description=cereal_desig, arms=[pickup_arm], grasps=[\"front\"]).resolve().perform()\n",
    "\n",
    "    ParkArmsAction([Arms.BOTH]).resolve().perform()\n",
    "\n",
    "    place_island = SemanticCostmapLocation(\"kitchen_island_surface\", kitchen_desig.resolve(), cereal_desig.resolve()).resolve()\n",
    "\n",
    "    place_stand = CostmapLocation(place_island.pose, reachable_for=robot_desig, reachable_arm=pickup_arm).resolve()\n",
    "\n",
    "    NavigateAction(target_locations=[place_stand.pose]).resolve().perform()\n",
    "\n",
    "    PlaceAction(cereal_desig, target_locations=[place_island.pose], arms=[pickup_arm]).resolve().perform()\n",
    "\n",
    "    ParkArmsAction([Arms.BOTH]).resolve().perform()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Trees\n",
    "Task trees are a hirachical representation of all Actions involved in a plan. The Task tree can later be used to inspect and restructre the execution order of Actions in the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycram.task\n",
    "import anytree\n",
    "tt = pycram.task.task_tree\n",
    "print(anytree.RenderTree(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anytree.dotexport import RenderTreeGraph, DotExporter\n",
    "RenderTreeGraph(tt).to_picture(\"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import sqlalchemy.orm\n",
    "import pycram.orm.base\n",
    "import pycram.orm.task\n",
    "import pycram.orm.object_designator\n",
    "import pycram.orm.motion_designator\n",
    "import pycram.orm.action_designator\n",
    "\n",
    "engine = sqlalchemy.create_engine(\"sqlite+pysqlite:///:memory:\", echo=False)\n",
    "session = sqlalchemy.orm.Session(bind=engine)\n",
    "pycram.orm.base.Base.metadata.create_all(engine)\n",
    "session.commit()\n",
    "\n",
    "\n",
    "tt.insert(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navigations = session.query(pycram.orm.action_designator.NavigateAction).all()\n",
    "print(*navigations, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navigations = session.query(pycram.orm.action_designator.NavigateAction, \n",
    "                            pycram.orm.base.Position, \n",
    "                            pycram.orm.base.Quaternion).\\\n",
    "                                join(pycram.orm.base.Position).\\\n",
    "                                join(pycram.orm.base.Quaternion).all()\n",
    "print(*navigations, sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
